## 神经网络训练

### WramUp | 预热学习率

* ResNet中提出

    * 训练开始时，选择较小学习率，训练了一些epoches后，模型趋于稳定

        > 开始时，权重初始化随机，若学习率较大，可能振荡

    * 此时，将学习率更改为预先设置的学习率，正式开始训练

* gradual warm up

    * 不使用常量预热，而是采用逐渐增大的学习率
    * 防止学习率突然变化的不足

### Gradient Accumulation

* 设置batch_size为10，共1000个数据 => 100 train steps，梯度更新100次

* 若显存不够，设batch_size为5，则第二个循环才达到10条，则每逢2的倍数才更新梯度，之后才将梯度清零 => 保持梯度更新100次

    <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201123135101289.png" alt="image-20201123135101289" style="zoom:80%;" />

## 语言模型的评价指标

#### PPL | Perplexity | 困惑度

* 基本思想

    * 测试集为正常的句子，能够给测试集较高概率的模型说明能准确判断是否成句，效果较好

* 公式：概率越大，PPL越小

    <img src="https://www.zhihu.com/equation?tex=PP%28W%29%3DP%28w_%7B1%7Dw_%7B2%7D...w_%7BN%7D%29%5E%7B-%5Cfrac%7B1%7D%7BN%7D%7D%3D%5Csqrt%5BN%5D%7B%5Cfrac%7B1%7D%7BP%28w_%7B1%7Dw_%7B2%7D...w_%7BN%7D%7D%7D" style="zoom: 80%;" >