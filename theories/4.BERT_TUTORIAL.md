# Overview

* **Step1：数据预处理**

    > tokenizer：编码器

    1. 分词
    2. 增加[CLS]和[SEP] 
    3. 得到token id
    4. 根据max_sentence_length，[PAD]或缩减至固定长
    5. 得到attention mask，真实词为1，[PAD]为0

    > 未提及segment id：将[SEP]间隔的两个句子，一个全1，一个全0。因为，此处为文本分类任务。
    >
    > [SEP]出现在问答任务或序列任务

* **Step2：训练过程**

    1. 初始化模型：确定from_pretrained、输出、运行GPU否
    2. 设置超参数、优化器以及学习率scheduler
    3. Training Loop
        * 设置种子、记录状态、初始时间
        * Training | 每epoch
            * 记录loss和初始时间，开启训练状态
            * 每xx batch，汇报进度
            * Unpack batch，得到b_input_ids, b_attention_masks, b_labels => 转GPU类型
            * 梯度清零
            * 计算损失 + 累加总损失
            * backward + 梯度裁剪 + 更新权值 + 更新学习率
            * 输出平均loss和运行时间
        * Validation
            * 记录accuracy、loss和初始时间；开启验证状态
            * Unpack + 转GPU
            * no_grad状态下，计算损失和logits
            * logits.detach()转CPU + labels转cpu => 计算accuracy
            * 计算平均损失、平均acc和运行时间
        * 输出总训练时间

* **Step3：测试集评估**

# 1 Tokenization & Input Formatting

> Tutorial：https://mccormickml.com/2019/07/22/BERT-fine-tuning/#31-bert-tokenizer

## 1.1 Bert Tokenizer

```python
import torch
from transformer import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# 分词
print('Tokenized: ', tokenizer.tokenize(sentences[0]))

# 得到词在词典中的对应ID
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))

# tokenize和convert_tokrns_to_ids => 由encode一步完成，先分词再得到ID
```

## 1.2 Tokenize Dataset

* **选择句子最大长度 | maximum sentence length**

    ```python
    max_len = 0
    
    for sent in sentences:
        # 需要算上特殊符号：得到每个词的对应id（列表形式）
    	input_ids = tokenizer.encode(sent, add_special_tokens=True)
        # 更新句子长度
        max_len = max(max_len, len(input_ids))
        
    print('Max sentence length: ', max_len)
    # 得到47，定最大长为64（保留一些空间，以防测试集遇到更长的句子）
    ```

* **使用encode_plus, 对每个句子作如下操作**  | max_len=64

    <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201123235510943.png" alt="image-20201123235510943" style="zoom: 67%;" />![image-20201124002752896](https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124002752896.png)

    <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124002752896.png" alt="image-20201124002752896" style="zoom:67%;" />

```python
input_ids = []
attention_masks = []

for sent in sentences:
    encode_dict = tokenizer.encode_plus(
    	sent,	# 需要编码的句子
        add_special_tokens=True,
        max_length=64,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt'		# 返回pytorch形式的张量
    )
    
    input_ids.append(encode_dict['input_ids'])
    attention_masks.append(encode_dict['attention_mask'])

# dataset x 64
# [PAD]的ID是0
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)		# 文本分类的label

print('Original: ', sentences[0])
print('Token IDs: ', input_ids[0])
```

> Original:  Our friends won't buy this analysis, let alone the next one we propose.
> Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,
>          2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,
>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
>             0,     0,     0,     0])

## 1.3 Training & Validation Split

### Step1：划分训练集和验证集 | random_split

```python
from torch.utils.data import TensorDataset, random_split

# 当作按索引压缩成一个元组，合并成一个数据集
# dataset[0][0] = input_ids, [0][1]=attention, [0][2]=labels
dataset = TensorDataset(input_ids, attention_masks, labels)

# 划分训练集和验证集
# 算出train_size和val_size
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size

# 根据size得到dataset（随机采样）
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
```

### Step2：将数据集转换为DataLoader

* BERT作者推荐微调时的batch_size为16或32
    * 训练集中表示用于训练的样本数
    * 测试集中表示用于评价的样本数
* 取batch的方式
    * RandomSampler：随机取batch
    * SequentialSampler：按顺序取batch

```python
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 32

# RandomSapler表示不按顺序取batch，而是随机取batch
train_ldataoader = DataLoader(
	train_dataset,	# 源数据
    sampler = RandomSampler(train_dataset),
    batch_size = batch_size		# 按batch训练
)

validation_dataloader = DataLoader(
	val_dataset,
    sampler = SequantialSampler(val_dataset),
    batch_size = batch_size
)
```

# 2 Train Our Classification Model

## 2.1 BertForSequenceClassification

* Target：将BERT微调成文本分类模型

* Huggingface Transformers

    * 基于BERT，设计不同的top layers &o utput types

    <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124003151309.png" alt="image-20201124003151309" style="zoom:67%;" />

* BertForClassification = BERT + Linear layer

    > As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.

```python
from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top
model = BertForSequenceClassification.from_pretrained(
	'bert-base-uncased',	# 12层，全小写vocab
    num_labels = 2,			# 二分类问题
    output_attentions = False,	# 模型输出不必返回attention的权重
    output_hidden_states = False
)

# 在GPU上跑模型
model.cuda()
```

**【模型参数】**

* **list(model.named_parameters())**
    * 按列表形式组织，每个元素为元组形式
    * tup[0]表示参数名，tup[1]表示参数张量
* **Embedding Layer**

```python
# rows=vocab_size, cols=embedding_size
bert.embeddings.word_embeddings.weight                  (30522, 768)
# 一个sent最长512个词（512个位置），每个位置用768维向量表示
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)
```

* **First-Transformer**

```python
bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)

bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)
```

* **Output Layer**

```
bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
```

## 2.2 Optimizer & Learning Rate Scheduler

### 超参数选择

* **BERT作者推荐的微调超参数**
    * batch-size：16,32
    * learning rate（Adam）：5e-5，3e-5, 2e-5
    * num of epochs: 2, 3, 4
* **此次训练的超参数**
    * batch-size：32
    * lr：2e-5
    * epochs：4 （将会导致过拟合）
    * epsilon：1e-8，对优化器中所有除数增加epsilon，防止除以0

### Optimizer：AdamW

* W：表示weight decay

```python
optimizer = AdamW(
	model.parameters(),
    lr = 2e-5,	# default is 5e-5
    eps = 1e-8	# default is 1e-8
)
```

### WarmUp

* 预热期间：学习率从0线性增加到优化器的初始lr
* 预热之后：创建schedule，使学习率从优化器初始lr线性降低到0

<img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124013354153.png" alt="image-20201124013354153" style="zoom:67%;" />

```python
from transformers import get_linear_schedule_with_warmup

epochs = 4

# training samples: batch_size * num of batches
# training steps(参数更新次数)：num of batches * epochs
total_steps = len(training_dataloader) * epochs

# create the learning rate scheduler
scheduler = get_linear_schedule_with_warmup(
	optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)
```

## 2.3 Training Loop

### LOOP的两大阶段

* 每个LOOP = 训练阶段 + 评估阶段

    * **Trainiing**

        <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124014051982.png" alt="image-20201124014051982" style="zoom:67%;" />

    * **Evaluation**

        <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124014105626.png" alt="image-20201124014105626" style="zoom:67%;" />

### Loop的相关函数

* **accuracy**：用 numPy 或 tensor

```python
import numpy as np

def flat_accuracy(preds, labels):
    preds_flat = np.argmax(preds, dim=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(preds_flat == labels_flat) / len(labels_flat)
```

* **elapsed time**：运行时间，格式hh:mm:ss

```python
import time
import datetime

def format_time(elapsed):
    # 传入总运行秒数，转换为 hh:mm:ss
    # 1. 对秒数四舍五入
    elapsed_rounded = int(round(elapsed))
    
    # 2. 格式转换
    return str(datetime.timedelta(seconds=elapsed_rounded))
```

* **Training**

    * seed：采用跟Huggingface一样的42

    * torch.nn.utils.clip_grad_norm(params, norm)：梯度裁剪

        > * 作用：防止梯度消失/爆炸，当梯度大于/小于某阈值，用某个值直接替代
        >
        > * 参数
        >
        >     * norm：权值运行范围[-norm, norm] 
        >
        > * 源码
        >
        >     ```python
        >     def clip_grad_value_(parameters, clip_value):
        >         """Clips gradient of an iterable of parameters at specified value.
        >         Gradients are modified in-place.
        >     
        >         Arguments:
        >             parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
        >                 single Tensor that will have gradients normalized
        >             clip_value (float or int): maximum allowed value of the gradients.
        >                 The gradients are clipped in the range
        >         """
        >     
        >         if isinstance(parameters, torch.Tensor):
        >             parameters = [parameters]	# 将张量变为迭代器
        >         clip_value = float(clip_value)
        >         # filter，过滤掉params中不符合条件的元素
        >         for p in filter(lambda p: p.grad is not None, parameters):
        >         	# clamp表示缩进，将梯度的每个元素限定在[min, max]之间
        >             # <min的梯度被调整为min，大于max的梯度被调整为max
        >             p.grad.data.clamp_(min=-clip_value, max=clip_value)
        >     ```

### LOOP完整实现

```python
import random
import numpy as np

# set seed
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed(seed_val)

# 记录状态: 每个epoch的train和val loss，两者的准确率，每次训练的时间
training_stats = []

# 记录整个模型训练的时间
total_t0 = time.time()

for epoch_i in range(epochs):
    # ===========================
    #         Training
    # ===========================
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i+1, epochs))
    print('Training ...')
    
    # 需要记录每次epoch的运行时间
    t0 = time.time()
    # 记录每个epoch的总损失 => 计算平均损失
    total_train_loss = 0
    
    # 需要开启训练模式：改变状态
    model.train()
    
    for step, batch in enumerate(train_dataloader):
        
        # 0.进度报告：每更新40次，报告进度
        if step % 40 == 0 and not step == 0:
            elapsed = format_time(time.time() - t0)
            # 逗号表示以逗号分隔
            print('  Batch {:>5,} of {:>5,}.	Elapsed: {:}.'.format(step, len(train_dataloader, elapsed)))
            
        # Unpack：每个batch是3维，对应input_ids，attention和label
        # 转GPU
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        # 模型前馈之前，清零之前梯度
        model.zero_grad()
        
        # 模型预测：logits表示模型最后一层激活函数之前的输出
        loss, logits = model(
            b_input_ids,
            token_type_ids=None,
            attention_mask=b_input_mask,
            labels=b_labels)
        # 计算总损失
        total_train_loss += loss.item()
        
        # 反馈 -> 梯度裁剪 -> 更新权值 -> 更新学习率
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
    
    # 结束一个epoch，记录平均损失和总运行时间
    avg_train_loss = total_train_loss / len(train_dataloader)
    training_time = format_time(time.time() - t0)
    
    print("")
    # 冒号前的0，表示输出后面format列表中索引0的元素
    print("   Average training loss: {0:.2f}",format(avg_train_loss))
    print("   Training Epoch took: {:}".format(training_time))
    
    # ===================
    #     Validation
    # ===================
    print("")
    print("Running Validation ...")
    
    t0 = time.time()
    # 开启评估模式
    model.eval()
    
    # 需要追踪的变量
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0
    
    for batch in validation_dataloader:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        # 评估阶段需要关闭计算图，前馈时无需额外记录
        with torch.no_grad():
            (loss, logits) = model(
            	b_input_ids,
                token_type_ids=None,
                attention_mask=b_input_mask,
                labels=b_labels
            )
        
        # 记录损失
        total_eval_loss += loss.item()
        
        # 移回CPU计算准确率：detach表示副本，不影响网络的原参数
        logits = logits.detach().cpu().numpy()
        labels_id = b_labels.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, labels_id)
    
    # 结束对所有batch，记录平均损失和总运行时间
    avg_eval_accuracy = total_eval_accuracy / len(validation_dataloader)
    avg_eval_loss = total_eval_loss / len(validation_dataloader)
    validation_time = format_time(time.time() - t0)
    
    # 冒号前的0，表示输出后面format列表中索引0的元素
    print("   Accuracy: {0:.2f}".format(avg_val_accuracy))
    print("   Validation loss: {0:.2f}",format(avg_val_loss))
    print("   Validation took: {:}".format(validation_time))
    
    training_stats.append(
    	{
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )
    
print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))
```

### LOOP输出形式

<img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124025547738.png" alt="image-20201124025547738" style="zoom:80%;" />

### 输出到表格

```python
import pandas as pd

# 设置表格中数字的精度为2
pd.set_option('precision', 2)

df_stats = pd.DataFrame(data=training_stats)
df_stats = df_stats.set_index('epoch')	# 设置横坐标
print(df_stats)
```

<img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124025815939.png" alt="image-20201124025815939" style="zoom:80%;" />

> 可见到epoch4，已经过拟合；
>
> 数据集：7695训练样本，856验证样本

### 绘制训练过程loss变化

```python
import matplotlib.pyplot as plt
% matplotlib inline

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(df_stats['Training Loss'], 'b-o', label="Training")
plt.plot(df_stats['Valid. Loss'], 'g-o', label="Validation")

# Label the plot.
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([1, 2, 3, 4])

plt.show()
```

## 3 Performence On Test Set

* 一般在这一步增加评测指标

### 3.1 Data Preperation

```python
# 用pandas或json读入数据

input_ids = []
attention_masks = []

for sent in sentences:
    
   	encode_dict = tokenizer.encode_plus(
    	sent,
        add_special_tokens=True,
        max_length=64,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    
    input_ids.append(encode_dict['input_ids'])
    attention_masks.append(encode_dict['attention_mask'])
    
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_maska, dim=0)
labels = torch.tensor(labels)

# 获取DataLoader
batch_size = 32

prediction_dataset = TensorDataset(input_ids, attention_masks, labels)
prediction_dataloader = DataLoader(
	prediction_dataset,
    sampler=SequencialSampler(prediction_dataset),
    batch_size=batch_size
)
```

### 3.2  Evaluate on Test set

* 文本分类的评价指标：[Matthew’s correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)
* 为了提高效果，调完参后撤掉验证集，对整个训练集进行训练

```python
print('Predicting labels for {:,} test sentences'.format(len(input_ids)))

model.eval()

# Tracking Variables
predictions, true_labels = [], []

# 预测
for batch in prediction_dataloader:
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask, b_labels = batch
    
    with torch.no_grad():
        # 用元组形式(loss, logits)时，才输入labels
        outputs = model(b,input_ids, token_type_id=None, attention_mask=b_input_mask)
        
    logits = outputs[0]
    
    # 转移至CPU格式，并存储
    logits = logits.detach().cpu().numpy()
    labels = b_labels.to('cpu').numpy()
    predicitons.append(logits)
    true_labels.append(labels)

print('      DONE.')
```

## 4 Saving & Loading Fine-Tuned Model

### Save Model

* 需要保存三种文件类型才能重新加载经过微调的模型
    * 模型权重文件：pytorch_model.bin (PyTorch序列化保存)
    * 配置文件：config.json
    * 词汇文件：vocab.txt（BERT和Transformer-XL）或 vocab.json（GPT/GPT-2的BPE词汇）
    * GPT/GPT-2的额外合并文件：merges.txt

```python
import os

output_path = './model_save/'

if not os.path.exists(output_path):
    os.mkdir(output_path)

print(f'Saving model to {output_path}')
# 分布式模型可能封装在DistributedDataParallel或DataParallel中
model_to_save = model.module if hasattr(model, 'module')
model_to_save.save_pretrained(output_path)		# 保存模型和配置
tokenizer.save_pretrained(output_path)			# 保存编码器
# tokenizer.save_vocabulary(output_path)
# torch.save(args, os.path.join(output_path, 'training_args.bin'))	保存训练参数
```

<img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201124083723040.png" alt="image-20201124083723040" style="zoom: 80%;" />

### Load Model

```python
model = model_class.from_pretrained(output_path)
tokenizer=  tokenizer_class.from_pretrained(output_path)

model.to(device)
```

