<!-- TOC -->

- [GPT-2生成式多轮对话](#gpt-2生成式多轮对话)
    - [框架](#框架)
- [CDial-GPT](#cdial-gpt)
    - [模型选择](#模型选择)
    - [数据处理](#数据处理)
    - [训练过程](#训练过程)
- [DialoGPT](#dialogpt)

<!-- /TOC -->

## GPT-2生成式多轮对话

### 框架

<img src="https://img-blog.csdnimg.cn/2020021620210095.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2c1MzQ0NDE5MjE=,size_16,color_FFFFFF,t_70">

* **无监督语料库学习 | 左侧**

    * 目标：标准语言模型，前文状态下最大化当前词的概率

    * 公式：多注意力机制 + 多层transfomer

        <img src="https://img-blog.csdnimg.cn/20200216200304928.png" style="zoom:67%;" >

        * 对词的上文向量进行多头自注意力机制

        > U：词的上文向量（该词的前k个词）
        >
        > $W_e$：词嵌入矩阵，embedding
        >
        > $W_p$：词位置矩阵

* **Fine tuning：监督学习 | 右侧**

    * 将预训练模型的最终输出，作为输入传入线性输出层j计算$h^m_lW^y$

    * 问答系统的预处理：在一个文档环境z、一个问题q和一组可能答案{ak}之间加分隔符$

        输入为[z; q; $; ak]，模型计算每个ak的possibility => softmax

## CDial-GPT

**【编码方式】**

<img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201123090949440.png" alt="image-20201123090949440" style="zoom:67%;" />

### 模型选择

* 需要根据历史序列信息，预测出问题的答案 => 单向即可

* 一些输入，一个输出 => Seq2seq/GPT

* GPT

    <img src="https://gitee.com/WIN0624/document/raw/markdown-picture/img/image-20201123075527988.png" alt="image-20201123075527988" style="zoom:67%;" />

    <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy81ZmtuYjQxaWI5cUVaVXpTQTJ4czhLaFpGZ2hSSTlvUUVNbHZLazF5eGM3dGUwaEFkUlJ4aWI1RkhFVm84M0pNRzNkVVNEMGdCZnhpY1QwaHR1b2ljV3RLUlEvNjQw?x-oss-process=image/format,png" style="zoom: 50%;" >
    
    > 分隔符SEP表示句子的结束：前后的句子拥有不同的segment id。

### 数据处理

* 将所有历史对话拼接成单句文本，用[SEP]分隔，作为模型输入

* 相对位置编码：GPT的绝对位置编码有上限，而对话轮数可能是无限的，因此使用相对位置编码。使用NEZHA的预训练权重作为模型的初始化权重。

### 训练过程

* 预训练：基于中文小说中数据，训练12层GPT，5亿个词

* 优化器：AdamW + Noam学习率衰减

* 学习率预热：1 epoch，不高于6.25e-5

* 模型

    * 12层GPT/GPT2 + 12head

    * word embedding size：768

    * position embedding：513

    * batch size：8

    * gradient accumulation：64 

        > 每加载64个batch，更新梯度并清零

* 在已得到的模型上，用STC数据集微调
    * $GPT_{novel}$：30 epochs
    * 其余模型：10 epochs，same batch size和accumulation，其余超参数不变
    * 学习率：从6.25e-5衰减到0
    * decoding strategy：top-p + temperature=0.7

## DialoGPT